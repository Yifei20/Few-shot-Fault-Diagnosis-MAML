{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-way, K-shot few-Shot learning parameters\n",
    "ways=10             # Number of classes in a task\n",
    "shots=5             # Number of training examples per class (same for testing)\n",
    "# Meta-learning parameters\n",
    "meta_lr=0.001       # Outer loop learning rate\n",
    "fast_lr=0.1         # Inner loop learning rate\n",
    "adapt_steps=5       # Number of inner loop update steps\n",
    "meta_batch_size=32  # Number of tasks sampled per batch\n",
    "iterations=500        # Number of outer loop iterations\n",
    "# Cuda and random seed settings\n",
    "cuda=True\n",
    "seed=42\n",
    "# Dataset parameters (different domain means different working condition)\n",
    "train_domain=1      # For CWRU dataset: 0: 1797, 1: 1772, 2: 1750, 3: 1730\n",
    "valid_domain=2     \n",
    "test_domain=3\n",
    "# Path to the data directory\n",
    "data_dir_path='./data'\n",
    "\n",
    "# test_step = 50\n",
    "test_batch_size = 128\n",
    "display_step = 25\n",
    "expriment_name = 'MAML_CWRU_10w_5s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cwru import CWRU\n",
    "from utils import fast_adapt\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import learn2learn as l2l\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from learn2learn.data.transforms import (\n",
    "    FusedNWaysKShots,\n",
    "    LoadData,\n",
    "    RemapLabels,\n",
    "    ConsecutiveLabels,\n",
    ")\n",
    "# Set the logger\n",
    "\n",
    "# setlogger(os.path.join(\"./logs\", expriment_name + '.log'))\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "fh = logging.FileHandler(os.path.join(\"./logs\", expriment_name + '.log'))\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "# ch = logging.StreamHandler()\n",
    "# ch.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(message)s\",\n",
    "                                   datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "fh.setFormatter(formatter)\n",
    "# ch.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "# logger.addHandler(ch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Checking and Random Seed Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-08 20:42:42] Training MAML with CPU.\n"
     ]
    }
   ],
   "source": [
    "# Set the Random Seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set training device, using GPU if available\n",
    "if cuda and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device_count = torch.cuda.device_count()\n",
    "    device = torch.device('cuda')\n",
    "    logging.info('Training MAML with {} GPU(s).'.format(device_count))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    logging.info('Training MAML with CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Meta-data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7d/zny8jmrj715bf909l1np7dfr0000gn/T/ipykernel_50040/2317948366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Create Meta-Tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mlearn2learn/data/meta_dataset.pyx\u001b[0m in \u001b[0;36mlearn2learn.data.meta_dataset.MetaDataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mlearn2learn/data/meta_dataset.pyx\u001b[0m in \u001b[0;36mlearn2learn.data.meta_dataset.MetaDataset.create_bookkeeping\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Few-shot-Fault-Diagnosis-MAML/datasets/cwru.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create Datasets\n",
    "train_dataset = CWRU(train_domain,\n",
    "                        data_dir_path)\n",
    "\n",
    "valid_dataset = CWRU(valid_domain,\n",
    "                        data_dir_path)\n",
    "\n",
    "test_dataset = CWRU(test_domain,\n",
    "                    data_dir_path)\n",
    "\n",
    "# Create Meta-Datasets\n",
    "train_dataset = l2l.data.MetaDataset(train_dataset)\n",
    "valid_dataset = l2l.data.MetaDataset(valid_dataset)\n",
    "test_dataset = l2l.data.MetaDataset(test_dataset)\n",
    "\n",
    "# Create Meta-Tasks\n",
    "train_transforms = [\n",
    "    FusedNWaysKShots(train_dataset, n=ways, k=2*shots),\n",
    "    LoadData(train_dataset),\n",
    "    RemapLabels(train_dataset),\n",
    "    ConsecutiveLabels(train_dataset),\n",
    "]\n",
    "train_tasks = l2l.data.Taskset(\n",
    "    train_dataset,\n",
    "    task_transforms=train_transforms,\n",
    "    num_tasks=400,\n",
    ")\n",
    "\n",
    "valid_transforms = [\n",
    "    FusedNWaysKShots(valid_dataset, n=ways, k=2*shots),\n",
    "    LoadData(valid_dataset),\n",
    "    ConsecutiveLabels(valid_dataset),\n",
    "    RemapLabels(valid_dataset),\n",
    "]\n",
    "valid_tasks = l2l.data.Taskset(\n",
    "    valid_dataset,\n",
    "    task_transforms=valid_transforms,\n",
    "    num_tasks=100,\n",
    ")\n",
    "\n",
    "test_transforms = [\n",
    "    FusedNWaysKShots(test_dataset, n=ways, k=2*shots),\n",
    "    LoadData(test_dataset),\n",
    "    RemapLabels(test_dataset),\n",
    "    ConsecutiveLabels(test_dataset),\n",
    "]\n",
    "test_tasks = l2l.data.Taskset(\n",
    "    test_dataset,\n",
    "    task_transforms=test_transforms,\n",
    "    num_tasks=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = l2l.vision.models.CNN4(output_size=10)\n",
    "model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n",
    "opt = torch.optim.Adam(model.parameters(), meta_lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Acc and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "train_err_list = []\n",
    "valid_err_list = []\n",
    "\n",
    "test_acc_list = []\n",
    "test_err_list = []\n",
    "\n",
    "best_valid_err = float('inf')\n",
    "best_train_err = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7d/zny8jmrj715bf909l1np7dfr0000gn/T/ipykernel_39776/926519371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         evaluation_error, evaluation_accuracy = fast_adapt(batch,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                                             \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/Few-shot-Fault-Diagnosis-MAML/utils.py\u001b[0m in \u001b[0;36mfast_adapt\u001b[0;34m(batch, learner, loss, adaptation_steps, shots, ways, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Evaluate the adapted model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/learn2learn/algorithms/maml.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, loss, first_order, allow_unused, allow_nograd)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 gradients = grad(loss,\n\u001b[0m\u001b[1;32m    160\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                  \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecond_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    opt.zero_grad()\n",
    "    meta_train_err_sum = 0.0\n",
    "    meta_train_acc_sum = 0.0\n",
    "    meta_valid_err_sum = 0.0\n",
    "    meta_valid_acc_sum = 0.0\n",
    "\n",
    "    for task in range(meta_batch_size):\n",
    "        # Compute meta-training loss\n",
    "        learner = maml.clone()\n",
    "        batch = train_tasks.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adapt_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        evaluation_error.backward()\n",
    "        meta_train_err_sum += evaluation_error.item()\n",
    "        meta_train_acc_sum += evaluation_accuracy.item()\n",
    "        \n",
    "        # Compute meta-validation loss\n",
    "        learner = maml.clone()\n",
    "        batch = valid_tasks.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adapt_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        meta_valid_err_sum += evaluation_error.item()\n",
    "        meta_valid_acc_sum += evaluation_accuracy.item()\n",
    "\n",
    "    # Train\n",
    "    meta_train_acc = meta_train_acc_sum / meta_batch_size\n",
    "    meta_train_err = meta_train_err_sum / meta_batch_size\n",
    "    # Valid\n",
    "    meta_valid_acc = meta_valid_acc_sum / meta_batch_size\n",
    "    meta_valid_err = meta_valid_err_sum / meta_batch_size\n",
    "\n",
    "    if meta_valid_err <= best_valid_err and meta_train_err <= best_train_err:\n",
    "        best_valid_err = meta_valid_err\n",
    "        best_train_err = meta_train_err\n",
    "        if not os.path.exists(\"./models\"):\n",
    "            os.makedirs(\"./models\")\n",
    "        torch.save(model.state_dict(), './models/' + expriment_name + '_best.pth')\n",
    "\n",
    "    train_acc_list.append(meta_train_acc)\n",
    "    train_err_list.append(meta_train_err)\n",
    "    valid_acc_list.append(meta_valid_acc)\n",
    "    valid_err_list.append(meta_valid_err)\n",
    "\n",
    "    # Print some metrics\n",
    "    # print('\\n')\n",
    "    logging.info('Iteration {}:'.format(iteration))\n",
    "    logging.info('Meta Train Error: {}.'.format(meta_train_err))\n",
    "    logging.info('Meta Train Accuracy: {}.'.format(meta_train_acc))\n",
    "    logging.info('Meta Valid Error: {}.'.format(meta_valid_err))\n",
    "    logging.info('Meta Valid Accuracy: {}.'.format(meta_valid_acc))\n",
    "\n",
    "    # ========================= plot ==========================\n",
    "    if (iteration % display_step == 0 and iteration != 0):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(train_acc_list, '-o', label=\"train acc\")\n",
    "        plt.plot(valid_acc_list, '-o', label=\"valid acc\")\n",
    "        plt.xlabel('Trainin iteration')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(\"Accuracy Curve by Iteration\")\n",
    "        plt.legend()\n",
    "        plt.subplot(122)\n",
    "        plt.plot(train_err_list, '-o', label=\"train loss\")\n",
    "        plt.plot(valid_err_list, '-o', label=\"valid loss\")\n",
    "        plt.xlabel('Trainin iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(\"Loss Curve by Iteration\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(\"CWRU Bearing Fault Diagnosis {}way-{}shot\".format(ways, shots))\n",
    "        plt.savefig('./results/' + expriment_name + '_{}.png'.format(iteration))\n",
    "        plt.show()\n",
    "\n",
    "    # Average the accumulated gradients and optimize\n",
    "    for p in model.parameters():\n",
    "        p.grad.data.mul_(1.0 / meta_batch_size)\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute meta-testing loss\n",
    "meta_test_error = 0.0\n",
    "meta_test_accuracy = 0.0\n",
    "model.load_state_dict(torch.load('./models/' + expriment_name + '_best.pth'))\n",
    "for task in range(test_batch_size):\n",
    "    learner = maml.clone()\n",
    "    batch = test_tasks.sample()\n",
    "    evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                        learner,\n",
    "                                                        loss,\n",
    "                                                        adapt_steps,\n",
    "                                                        shots,\n",
    "                                                        ways,\n",
    "                                                        device)\n",
    "    meta_test_error += evaluation_error.item()\n",
    "    meta_test_accuracy += evaluation_accuracy.item()\n",
    "# print('\\n')\n",
    "logging.info('Meta Test Error: {}.'.format(meta_test_error / meta_batch_size))\n",
    "logging.info('Meta Test Accuracy: {}.\\n'.format(meta_test_accuracy / meta_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
