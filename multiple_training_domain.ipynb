{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cwru import CWRU\n",
    "from utils import (setlogger, fast_adapt)\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import learn2learn as l2l\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from learn2learn.data.transforms import (\n",
    "    FusedNWaysKShots,\n",
    "    LoadData,\n",
    "    RemapLabels,\n",
    "    ConsecutiveLabels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-way, K-shot few-Shot learning parameters\n",
    "ways=10             # Number of classes in a task\n",
    "shots=5             # Number of training examples per class (same for testing)\n",
    "# Meta-learning parameters\n",
    "meta_lr=0.001       # Outer loop learning rate\n",
    "fast_lr=0.1         # Inner loop learning rate\n",
    "adapt_steps=5       # Number of inner loop update steps\n",
    "meta_batch_size=32  # Number of tasks sampled per batch\n",
    "iterations=5        # Number of outer loop iterations\n",
    "# Cuda and random seed settings\n",
    "cuda=True\n",
    "seed=42\n",
    "# Dataset parameters (different domain means different working condition)\n",
    "train_domain1=0      # For CWRU dataset: 0: 1797, 1: 1772, 2: 1750, 3: 1730\n",
    "train_domain2=1\n",
    "\n",
    "valid_domain=2     \n",
    "test_domain=3\n",
    "# Path to the data directory\n",
    "data_dir_path='./data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Random Seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set training device, using GPU if available\n",
    "if cuda and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device_count = torch.cuda.device_count()\n",
    "    device = torch.device('cuda')\n",
    "    logging.info('Training MAML with {} GPU(s).'.format(device_count))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    logging.info('Training MAML with CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_dataset1 = CWRU(train_domain1,\n",
    "                        data_dir_path)\n",
    "train_dataset2 = CWRU(train_domain2,\n",
    "                        data_dir_path)\n",
    "\n",
    "valid_dataset = CWRU(valid_domain,\n",
    "                        data_dir_path)\n",
    "\n",
    "test_dataset = CWRU(test_domain,\n",
    "                    data_dir_path)\n",
    "\n",
    "# Create Meta-Datasets\n",
    "train_dataset1 = l2l.data.MetaDataset(train_dataset1)\n",
    "train_dataset2 = l2l.data.MetaDataset(train_dataset2)\n",
    "valid_dataset = l2l.data.MetaDataset(valid_dataset)\n",
    "test_dataset = l2l.data.MetaDataset(test_dataset)\n",
    "\n",
    "# Create Meta-Tasks\n",
    "train_transforms = [\n",
    "    FusedNWaysKShots(train_dataset1, n=ways, k=2*shots),\n",
    "    LoadData(train_dataset1),\n",
    "    RemapLabels(train_dataset1),\n",
    "    ConsecutiveLabels(train_dataset1),\n",
    "]\n",
    "train_tasks1 = l2l.data.Taskset(\n",
    "    train_dataset1,\n",
    "    task_transforms=train_transforms,\n",
    "    num_tasks=400,\n",
    ")\n",
    "train_transforms = [\n",
    "    FusedNWaysKShots(train_dataset2, n=ways, k=2*shots),\n",
    "    LoadData(train_dataset2),\n",
    "    RemapLabels(train_dataset2),\n",
    "    ConsecutiveLabels(train_dataset2),\n",
    "]\n",
    "train_tasks2 = l2l.data.Taskset(\n",
    "    train_dataset2,\n",
    "    task_transforms=train_transforms,\n",
    "    num_tasks=400,\n",
    ")\n",
    "\n",
    "valid_transforms = [\n",
    "    FusedNWaysKShots(valid_dataset, n=ways, k=2*shots),\n",
    "    LoadData(valid_dataset),\n",
    "    ConsecutiveLabels(valid_dataset),\n",
    "    RemapLabels(valid_dataset),\n",
    "]\n",
    "valid_tasks = l2l.data.Taskset(\n",
    "    valid_dataset,\n",
    "    task_transforms=valid_transforms,\n",
    "    num_tasks=100,\n",
    ")\n",
    "\n",
    "test_transforms = [\n",
    "    FusedNWaysKShots(test_dataset, n=ways, k=2*shots),\n",
    "    LoadData(test_dataset),\n",
    "    RemapLabels(test_dataset),\n",
    "    ConsecutiveLabels(test_dataset),\n",
    "]\n",
    "test_tasks = l2l.data.Taskset(\n",
    "    test_dataset,\n",
    "    task_transforms=test_transforms,\n",
    "    num_tasks=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = l2l.vision.models.CNN4(output_size=10)\n",
    "model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n",
    "opt = torch.optim.Adam(model.parameters(), meta_lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, iterations+1):\n",
    "    opt.zero_grad()\n",
    "    meta_train_error = 0.0\n",
    "    meta_train_accuracy = 0.0\n",
    "    meta_valid_error = 0.0\n",
    "    meta_valid_accuracy = 0.0\n",
    "    meta_test_error = 0.0\n",
    "    meta_test_accuracy = 0.0\n",
    "\n",
    "    for task in range(meta_batch_size):\n",
    "        # Compute meta-training loss\n",
    "        learner = maml.clone()\n",
    "        batch = train_tasks1.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adapt_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        evaluation_error.backward()\n",
    "        meta_train_error += evaluation_error.item()\n",
    "        meta_train_accuracy += evaluation_accuracy.item()\n",
    "        \n",
    "        learner = maml.clone()\n",
    "        batch = train_tasks2.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adapt_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        evaluation_error.backward()\n",
    "        meta_train_error += evaluation_error.item()\n",
    "        meta_train_accuracy += evaluation_accuracy.item()\n",
    "        \n",
    "        # Compute meta-validation loss\n",
    "        learner = maml.clone()\n",
    "        batch = valid_tasks.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adapt_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        meta_valid_error += evaluation_error.item()\n",
    "        meta_valid_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # # Compute meta-testing loss\n",
    "        # learner = maml.clone()\n",
    "        # batch = test_tasks.sample()\n",
    "        # evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "        #                                                     learner,\n",
    "        #                                                     loss,\n",
    "        #                                                     adapt_steps,\n",
    "        #                                                     shots,\n",
    "        #                                                     ways,\n",
    "        #                                                     device)\n",
    "        # meta_test_error += evaluation_error.item()\n",
    "        # meta_test_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "    train_acc.append(meta_train_accuracy / meta_batch_size)\n",
    "    valid_acc.append(meta_valid_accuracy / meta_batch_size)\n",
    "    train_loss.append(meta_train_error / meta_batch_size)\n",
    "    valid_loss.append(meta_valid_error / meta_batch_size)\n",
    "\n",
    "    # Print some metrics\n",
    "    # if (iteration % 10 == 0):\n",
    "    print('\\n')\n",
    "    print('Iteration', iteration)\n",
    "    print('Meta Train Error', meta_train_error / meta_batch_size)\n",
    "    print('Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n",
    "    print('Meta Valid Error', meta_valid_error / meta_batch_size)\n",
    "    print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n",
    "    print('Meta Test Error', meta_test_error / meta_batch_size)\n",
    "    print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n",
    "\n",
    "    # ========================= plot ==========================\n",
    "    if (iteration % 1 == 0):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(train_acc, '-o', label=\"train acc\")\n",
    "        plt.plot(valid_acc, '-o', label=\"valid acc\")\n",
    "        plt.xlabel('Trainin iteration')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(\"Accuracy Curve by Iteration\")\n",
    "        plt.legend()\n",
    "        plt.subplot(122)\n",
    "        plt.plot(train_loss, '-o', label=\"train loss\")\n",
    "        plt.plot(valid_loss, '-o', label=\"valid loss\")\n",
    "        plt.xlabel('Trainin iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(\"Loss Curve by Iteration\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(\"CWRU Bearing Fault Diagnosis {}way-{}shot\".format(ways, shots))\n",
    "        # plt.savefig('./results/image_{}.png'.format(iteration))\n",
    "        plt.show()\n",
    "\n",
    "    # Average the accumulated gradients and optimize\n",
    "    for p in model.parameters():\n",
    "        p.grad.data.mul_(1.0 / meta_batch_size)\n",
    "    opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
